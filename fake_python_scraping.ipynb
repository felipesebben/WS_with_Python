{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi, everyone! In this notebook, I will scrape some data from Fake Python job postings, an example website provided by [Real Python](https://realpython.com) for practicing. As this is pretty much an exercise on what has been [posted](https://realpython.com/beautiful-soup-web-scraper-python/) on their website, I would like to thank Martin Breuss for putting together the tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, I will build a web scraper that fetches Python **software developer** job listings from the [Fake Python Jobs](https://https://realpython.github.io/fake-jobs/) site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and retrieve the HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "URL = \"https://realpython.github.io/fake-jobs/\"\n",
    "page = requests.get(URL)\n",
    "\n",
    "print(page.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Beautiful Soup object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, 'html.parser') #page.content (HTML) taken as input\n",
    "                                                  #'html.parser' makes sure you use the right parser for HTML content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find elements by ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look for the element \"ResultsContainer\", which contains all the job postings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = soup.find(id='ResultsContainer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also use `prettify()` to provide an easier viewing of the Beautiful Soup object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Elements by HTML Class Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All job postings are wrapped in a `<div>` element with the class `card-content`. Let us know use `results` and select only the job postings in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_elements = results.find_all('div', class_='card-content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.find_all()` returns an iterable containing all the HTML for all the avaliable job listings on the page. Let us take a look at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_element in job_elements:\n",
    "    print(job_element, end='\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, too much HTML. Let us pick out the elements we want from each job posting with `.find()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_element in job_elements:\n",
    "    title_element = job_element.find('h2', class_='title')\n",
    "    company_element = job_element.find('h3', class_='company')\n",
    "    location_element = job_element.find('p', class_='location')\n",
    "    print(title_element)\n",
    "    print(company_element)\n",
    "    print(location_element)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `job_element` is another `BeautifulSoup()` object, which means that this method can be used on the parent element, `results`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Text From HTML Elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add `.text` to a Beautiful Soup object to return **only** the text content of the HTML elements that the object contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_element in job_elements:\n",
    "    title_element = job_element.find('h2', class_='title')\n",
    "    company_element = job_element.find('h3', class_='company')\n",
    "    location_element = job_element.find('p', class_='location')\n",
    "    print(title_element.text)\n",
    "    print(company_element.text)\n",
    "    print(location_element.text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get rid of the extra **whitespace** with `.strip()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_element in job_elements:\n",
    "    title_element = job_element.find('h2', class_='title')\n",
    "    company_element = job_element.find('h3', class_='company')\n",
    "    location_element = job_element.find('p', class_='location')\n",
    "    print(title_element.text.strip())\n",
    "    print(company_element.text.strip())\n",
    "    print(location_element.text.strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Elements by Class Name and Text Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still looking for positions as software developer, therefore we need to filter the results. Since titles are kept within `<h2>` elements, we can use the `string` argument to get specific outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_jobs = results.find_all('h2', string='Python') #finds all <h2> elements where the contained string matches \"Python\" exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(python_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, why is the result empty? That is because `string=` is spelling, whitespace, and case sensitive, which means that any difference will impinge on the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass a Function to a Beautiful Soup Method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can sometimes pass **functions** as arguments to Beautiful Soup methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_jobs = results.find_all(\n",
    "    'h2', string=lambda text: 'python' in text.lower()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an **anonymous function** to the `string=` argument. The **lambda function** looks at the text of each `<h2>` element, converts it to lowercase, and checks whether the substring \"python\" is found anywhere. Let us check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(python_jobs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have found 10 jobs that include the word \"python\" in their job title. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Parent Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stepping up in the hierarchy of the DOM starting from the identified `<h2>` elements may help. Let us find the `<h2>` element that contains the **job title** as well as its **closest parent element** that contains all the information that youâ€™re interested in. In this case, we are looking for the `<div>` element with the `card-content` class, which contains all the information we need. We can now use `python_jobs` to fetch their great-grandparent elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_jobs = results.find_all(\n",
    "    'h2', string=lambda text: 'python' in text.lower()\n",
    ")\n",
    "\n",
    "python_job_elements = [\n",
    "    h2_element.parent.parent.parent for h2_element in python_jobs\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a **list comprehension** that works on each of the `<h2>` title elements in `python_jobs` that were retrieved by filtering with the lambda expression. Let us now adapt the code to the `for` loop to iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_element in python_job_elements:\n",
    "    title_element = job_element.find('h2', class_='title')\n",
    "    company_element = job_element.find('h3', class_='company')\n",
    "    location_element = job_element.find('p', class_='location')\n",
    "    print(title_element.text.strip())\n",
    "    print(company_element.text.strip())\n",
    "    print(location_element.text.strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Attributes From HTML Elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to scrape the link to apply for a job. However, if we try to find link elements in the same way as we have been doing so far, we will not retrieve the URLs for which we are looking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_element in python_job_elements:\n",
    "    title_element = job_element.find('h2', class_='title')\n",
    "    company_element = job_element.find('h3', class_='company')\n",
    "    location_element = job_element.find('p', class_='location')\n",
    "    links = job_element.find_all('a')\n",
    "    for link in links:\n",
    "        print(link.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See? The results display only the link texts **Learn** and **Apply** instead of the associated URLs. And why so? Because the `.text` attribute displays only the **visible** content of an HTML content: all attributes, including those containing URLs, are stripped away. We need to extract the value of one of the HTML attributes.\n",
    "We can do that by looking for the `href` attribute at the HTML of a single job posting. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<!-- snip -->\n",
    "    <footer class=\"card-footer\">\n",
    "        <a href=\"https://www.realpython.com\" target=\"_blank\"\n",
    "           class=\"card-footer-item\">Learn</a>\n",
    "        <a href=\"https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html\"\n",
    "           target=\"_blank\"\n",
    "           class=\"card-footer-item\">Apply</a>\n",
    "    </footer>\n",
    "  </div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can extract the value of these `href` attributes in each `<a>` element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_element in job_elements:\n",
    "    title_element = job_element.find('h2', class_='title')\n",
    "    company_element = job_element.find('h3', class_='company')\n",
    "    location_element = job_element.find('p', class_='location')\n",
    "    print(title_element.text.strip())\n",
    "    print(company_element.text.strip())\n",
    "    print(location_element.text.strip())\n",
    "    links = job_element.find_all('a')\n",
    "    for link in links:\n",
    "        link_url = link['href']\n",
    "        print(f\"Apply here: {link_url}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we fetched all links from the job postings we had previously filtered. We the extracted the `href` attribute contaning URLs. Still, not all the links are useful to our query. Let us refine our results a little bit further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise - how do we retreive only the URL of the *second* link?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_element in python_job_elements:\n",
    "    title_element = job_element.find('h2', class_='title')\n",
    "    company_element = job_element.find('h3', class_='company')\n",
    "    location_element = job_element.find('p', class_='location')\n",
    "    link_url = job_element.find_all('a')[1]['href']\n",
    "    print(title_element.text.strip())\n",
    "    print(company_element.text.strip())\n",
    "    print(location_element.text.strip())\n",
    "    print(f\"Apply here: {link_url}\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it! Awesome! We have managed to scrape valuable data from a bunch of job postings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_lists = []\n",
    "for job_element in python_job_elements:\n",
    "    list_of_page = []\n",
    "    title_element = job_element.find('h2', class_='title')\n",
    "    company_element = job_element.find('h3', class_='company')\n",
    "    location_element = job_element.find('p', class_='location')\n",
    "    link_url = job_element.find_all('a')[1]['href']\n",
    "        \n",
    "            \n",
    "    list_of_page.append(title_element)\n",
    "    list_of_page.append(company_element)\n",
    "    list_of_page.append(location_element)\n",
    "    list_of_page.append(link_url)\n",
    "    list_of_lists.append(list_of_page)\n",
    "\n",
    "df = pd.DataFrame(list_of_lists, columns=['v_1', 'v_2', 'v_3', 'v_4'])\n",
    "\n",
    "df.to_csv('jobs-list.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that was a botched attempt to store retrieved data as a `.csv` file. That will be subject of another notebook! Thank you again Martin Breuss and [Real Python](https://realpython.com) for the amazing content that you guys provide everyday!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0ec049a9b44b67c03c5e8314e6b8ef566c788a0c660ad59bb85840c27c269301"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
